{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# //// REFERENCES USED\n",
    "# https://datascience.stackexchange.com/questions/17282/xgbregressor-vs-xgboost-train-huge-speed-difference\n",
    "# //xgboost ultimate guide\n",
    "# https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n",
    "# https://datascience.stackexchange.com/questions/13557/xgboost-increase-the-error-when-changing-evaluation-function\n",
    "# https://stackoverflow.com/questions/32377040/xgboost-watchlist-parameter-dmatrix-object-is-not-iterable\n",
    "# https://stackoverflow.com/questions/51587535/custom-evaluation-function-based-on-f1-for-use-in-xgboost-python-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_final_matrix_factorized2.csv')\n",
    "test = pd.read_csv('test_final_matrix_factorized2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>user_mention</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0_x</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>...</th>\n",
       "      <th>64_y.1</th>\n",
       "      <th>65_y.1</th>\n",
       "      <th>66_y.1</th>\n",
       "      <th>67_y.1</th>\n",
       "      <th>68_y.1</th>\n",
       "      <th>69_y.1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>hastags_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.152920</td>\n",
       "      <td>0.024042</td>\n",
       "      <td>-0.013365</td>\n",
       "      <td>-0.015425</td>\n",
       "      <td>-0.028913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042019</td>\n",
       "      <td>0.053804</td>\n",
       "      <td>-0.028318</td>\n",
       "      <td>-0.130021</td>\n",
       "      <td>0.047160</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031551</td>\n",
       "      <td>-0.032375</td>\n",
       "      <td>-0.014239</td>\n",
       "      <td>-0.021525</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.039123</td>\n",
       "      <td>0.174968</td>\n",
       "      <td>-0.049157</td>\n",
       "      <td>-0.119076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.239905</td>\n",
       "      <td>0.883869</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>-0.005283</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027979</td>\n",
       "      <td>-0.026474</td>\n",
       "      <td>-0.011341</td>\n",
       "      <td>-0.020703</td>\n",
       "      <td>-0.012069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.046966</td>\n",
       "      <td>0.068540</td>\n",
       "      <td>0.103433</td>\n",
       "      <td>-0.011487</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stopwords  numerics  upper  user_mention  sentiment       0_x       1_x  \\\n",
       "0         10         0      0             1       -0.5  0.152920  0.024042   \n",
       "1          5         0      0             2        0.0 -0.031551 -0.032375   \n",
       "2          1         0      0             0        0.0 -0.005010 -0.039123   \n",
       "3          5         0      0             0        0.5 -0.239905  0.883869   \n",
       "4          1         0      0             0        0.0 -0.027979 -0.026474   \n",
       "\n",
       "        2_x       3_x       4_x  ...    64_y.1    65_y.1    66_y.1    67_y.1  \\\n",
       "0 -0.013365 -0.015425 -0.028913  ...  0.042019  0.053804 -0.028318 -0.130021   \n",
       "1 -0.014239 -0.021525 -0.012394  ... -0.001445 -0.001088  0.000287  0.001595   \n",
       "2  0.174968 -0.049157 -0.119076  ... -0.001445 -0.001088  0.000287  0.001595   \n",
       "3  0.018206  0.018790  0.012645  ...  0.002151 -0.002372 -0.000285  0.000318   \n",
       "4 -0.011341 -0.020703 -0.012069  ... -0.032624  0.017314  0.046966  0.068540   \n",
       "\n",
       "     68_y.1    69_y.1  word_count  char_count  hastags_count  label  \n",
       "0  0.047160  0.000264           6          53              1      0  \n",
       "1  0.000112 -0.000503           8          47              3      0  \n",
       "2  0.000112 -0.000503           2          14              0      0  \n",
       "3 -0.005283  0.001909           4          18              1      0  \n",
       "4  0.103433 -0.011487           2          18              1      0  \n",
       "\n",
       "[5 rows x 349 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>user_mention</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0_x</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2_x</th>\n",
       "      <th>3_x</th>\n",
       "      <th>4_x</th>\n",
       "      <th>...</th>\n",
       "      <th>65_y.1</th>\n",
       "      <th>66_y.1</th>\n",
       "      <th>67_y.1</th>\n",
       "      <th>68_y.1</th>\n",
       "      <th>69_y.1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>hastags_count</th>\n",
       "      <th>label</th>\n",
       "      <th>us_mn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.152920</td>\n",
       "      <td>0.024042</td>\n",
       "      <td>-0.013365</td>\n",
       "      <td>-0.015425</td>\n",
       "      <td>-0.028913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053804</td>\n",
       "      <td>-0.028318</td>\n",
       "      <td>-0.130021</td>\n",
       "      <td>0.047160</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031551</td>\n",
       "      <td>-0.032375</td>\n",
       "      <td>-0.014239</td>\n",
       "      <td>-0.021525</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.039123</td>\n",
       "      <td>0.174968</td>\n",
       "      <td>-0.049157</td>\n",
       "      <td>-0.119076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.239905</td>\n",
       "      <td>0.883869</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>-0.005283</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.027979</td>\n",
       "      <td>-0.026474</td>\n",
       "      <td>-0.011341</td>\n",
       "      <td>-0.020703</td>\n",
       "      <td>-0.012069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.046966</td>\n",
       "      <td>0.068540</td>\n",
       "      <td>0.103433</td>\n",
       "      <td>-0.011487</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stopwords  numerics  upper  user_mention  sentiment       0_x       1_x  \\\n",
       "0         10         0      0             1       -0.5  0.152920  0.024042   \n",
       "1          5         0      0             2        0.0 -0.031551 -0.032375   \n",
       "2          1         0      0             0        0.0 -0.005010 -0.039123   \n",
       "3          5         0      0             0        0.5 -0.239905  0.883869   \n",
       "4          1         0      0             0        0.0 -0.027979 -0.026474   \n",
       "\n",
       "        2_x       3_x       4_x  ...    65_y.1    66_y.1    67_y.1    68_y.1  \\\n",
       "0 -0.013365 -0.015425 -0.028913  ...  0.053804 -0.028318 -0.130021  0.047160   \n",
       "1 -0.014239 -0.021525 -0.012394  ... -0.001088  0.000287  0.001595  0.000112   \n",
       "2  0.174968 -0.049157 -0.119076  ... -0.001088  0.000287  0.001595  0.000112   \n",
       "3  0.018206  0.018790  0.012645  ... -0.002372 -0.000285  0.000318 -0.005283   \n",
       "4 -0.011341 -0.020703 -0.012069  ...  0.017314  0.046966  0.068540  0.103433   \n",
       "\n",
       "     69_y.1  word_count  char_count  hastags_count  label  us_mn  \n",
       "0  0.000264           6          53              1      0      0  \n",
       "1 -0.000503           8          47              3      0      0  \n",
       "2 -0.000503           2          14              0      0      0  \n",
       "3  0.001909           4          18              1      0      0  \n",
       "4 -0.011487           2          18              1      0      0  \n",
       "\n",
       "[5 rows x 350 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['us_mn'] = train['user_mention'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "test['us_mn'] = test['user_mention'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_final.csv', index = False)\n",
    "test.to_csv('test_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15201acb1d0>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(y = 'user_mention', x = 'label', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label']\n",
    "train.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.values\n",
    "y_train = y.values\n",
    "x_test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0,1))\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train, y, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = [(list(x_valid.values), list(y_valid.values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=1,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=10, missing=None, n_estimators=2, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0.3, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=False, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "x_train2 = xgb.DMatrix(x_train, y_train)\n",
    "dtrain = xgb.DMatrix(x_train, label=np.log1p(y_train))\n",
    "xg_cl = xgb.XGBClassifier(silent = False, reg_alpha = 0.3, objective = \"binary:logistic\",\n",
    "                          learning_rate = 0.1, subsample = 0.8, n_estimators = 2, \n",
    "                          min_child_weight = 10, max_depth = 8, colsample_bytree = 0.5 , gamma = 1)\n",
    "xg_cl.fit(x_train, y_train, eval_metric = f1_score, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5873221216041398\n"
     ]
    }
   ],
   "source": [
    "d_valid = xgb.DMatrix(x_valid.values, feature_names =x_valid.columns.values )\n",
    "y_pred = bst.predict(d_valid)\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "# print((y_pred))\n",
    "print(f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Level Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989,\\n            15990,\\n            ...\\n            31952, 31953, 31954, 31955, 31956, 31957, 31958, 31959, 31960,\\n            31961],\\n           dtype='int64', length=15981)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-ff9eda0e05c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_tran\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\safe\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2922\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2923\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[1;32m-> 2924\u001b[1;33m                                                    raise_missing=True)\n\u001b[0m\u001b[0;32m   2925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\safe\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[0;32m   1353\u001b[0m                           raise_missing}\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\safe\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[0;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\safe\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1244\u001b[0m                 raise KeyError(\n\u001b[0;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[1;32m-> 1246\u001b[1;33m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989,\\n            15990,\\n            ...\\n            31952, 31953, 31954, 31955, 31956, 31957, 31958, 31959, 31960,\\n            31961],\\n           dtype='int64', length=15981)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 2, random_state = 42, shuffle = False)\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    Y_train, Y_test = y_tran[train_index], y_train[test_index]\n",
    "    print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.243436\tvalid_0's f1: 0\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.23538\tvalid_0's f1: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\tvalid_0's binary_logloss: 0.228496\tvalid_0's f1: 0\n",
      "[4]\tvalid_0's binary_logloss: 0.222752\tvalid_0's f1: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tvalid_0's binary_logloss: 0.217467\tvalid_0's f1: 0\n",
      "[6]\tvalid_0's binary_logloss: 0.213245\tvalid_0's f1: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\tvalid_0's binary_logloss: 0.20944\tvalid_0's f1: 0\n",
      "[8]\tvalid_0's binary_logloss: 0.205564\tvalid_0's f1: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\nitin\\anaconda3\\envs\\safe\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\tvalid_0's binary_logloss: 0.202438\tvalid_0's f1: 0\n",
      "[10]\tvalid_0's binary_logloss: 0.198821\tvalid_0's f1: 0\n",
      "[11]\tvalid_0's binary_logloss: 0.195835\tvalid_0's f1: 0.0512821\n",
      "[12]\tvalid_0's binary_logloss: 0.192998\tvalid_0's f1: 0.0554371\n",
      "[13]\tvalid_0's binary_logloss: 0.190923\tvalid_0's f1: 0.157576\n",
      "[14]\tvalid_0's binary_logloss: 0.188748\tvalid_0's f1: 0.190099\n",
      "[15]\tvalid_0's binary_logloss: 0.186405\tvalid_0's f1: 0.210526\n",
      "[16]\tvalid_0's binary_logloss: 0.184109\tvalid_0's f1: 0.227799\n",
      "[17]\tvalid_0's binary_logloss: 0.181835\tvalid_0's f1: 0.234615\n",
      "[18]\tvalid_0's binary_logloss: 0.179751\tvalid_0's f1: 0.258065\n",
      "[19]\tvalid_0's binary_logloss: 0.178109\tvalid_0's f1: 0.272897\n",
      "[20]\tvalid_0's binary_logloss: 0.176245\tvalid_0's f1: 0.27933\n",
      "[21]\tvalid_0's binary_logloss: 0.17467\tvalid_0's f1: 0.290976\n",
      "[22]\tvalid_0's binary_logloss: 0.173154\tvalid_0's f1: 0.306569\n",
      "[23]\tvalid_0's binary_logloss: 0.172017\tvalid_0's f1: 0.31216\n",
      "[24]\tvalid_0's binary_logloss: 0.170446\tvalid_0's f1: 0.317117\n",
      "[25]\tvalid_0's binary_logloss: 0.169148\tvalid_0's f1: 0.324421\n",
      "[26]\tvalid_0's binary_logloss: 0.167676\tvalid_0's f1: 0.346154\n",
      "[27]\tvalid_0's binary_logloss: 0.166408\tvalid_0's f1: 0.359862\n",
      "[28]\tvalid_0's binary_logloss: 0.165309\tvalid_0's f1: 0.365517\n",
      "[29]\tvalid_0's binary_logloss: 0.164295\tvalid_0's f1: 0.372014\n",
      "[30]\tvalid_0's binary_logloss: 0.163419\tvalid_0's f1: 0.384486\n",
      "[31]\tvalid_0's binary_logloss: 0.16217\tvalid_0's f1: 0.39399\n",
      "[32]\tvalid_0's binary_logloss: 0.160911\tvalid_0's f1: 0.396667\n",
      "[33]\tvalid_0's binary_logloss: 0.159948\tvalid_0's f1: 0.406612\n",
      "[34]\tvalid_0's binary_logloss: 0.158878\tvalid_0's f1: 0.406612\n",
      "[35]\tvalid_0's binary_logloss: 0.157888\tvalid_0's f1: 0.427406\n",
      "[36]\tvalid_0's binary_logloss: 0.157382\tvalid_0's f1: 0.426016\n",
      "[37]\tvalid_0's binary_logloss: 0.156626\tvalid_0's f1: 0.438003\n",
      "[38]\tvalid_0's binary_logloss: 0.155899\tvalid_0's f1: 0.43871\n",
      "[39]\tvalid_0's binary_logloss: 0.155158\tvalid_0's f1: 0.443018\n",
      "[40]\tvalid_0's binary_logloss: 0.154301\tvalid_0's f1: 0.439807\n",
      "[41]\tvalid_0's binary_logloss: 0.153698\tvalid_0's f1: 0.442308\n",
      "[42]\tvalid_0's binary_logloss: 0.153322\tvalid_0's f1: 0.443381\n",
      "[43]\tvalid_0's binary_logloss: 0.152695\tvalid_0's f1: 0.455696\n",
      "[44]\tvalid_0's binary_logloss: 0.152099\tvalid_0's f1: 0.452532\n",
      "[45]\tvalid_0's binary_logloss: 0.15156\tvalid_0's f1: 0.452532\n",
      "[46]\tvalid_0's binary_logloss: 0.151242\tvalid_0's f1: 0.451817\n",
      "[47]\tvalid_0's binary_logloss: 0.150763\tvalid_0's f1: 0.459119\n",
      "[48]\tvalid_0's binary_logloss: 0.15022\tvalid_0's f1: 0.468019\n",
      "[49]\tvalid_0's binary_logloss: 0.149687\tvalid_0's f1: 0.47205\n",
      "[50]\tvalid_0's binary_logloss: 0.149222\tvalid_0's f1: 0.47678\n",
      "[51]\tvalid_0's binary_logloss: 0.148765\tvalid_0's f1: 0.483821\n",
      "[52]\tvalid_0's binary_logloss: 0.148398\tvalid_0's f1: 0.490798\n",
      "[53]\tvalid_0's binary_logloss: 0.147983\tvalid_0's f1: 0.491551\n",
      "[54]\tvalid_0's binary_logloss: 0.147606\tvalid_0's f1: 0.5\n",
      "[55]\tvalid_0's binary_logloss: 0.147427\tvalid_0's f1: 0.502283\n",
      "[56]\tvalid_0's binary_logloss: 0.147024\tvalid_0's f1: 0.504559\n",
      "[57]\tvalid_0's binary_logloss: 0.146598\tvalid_0's f1: 0.504559\n",
      "[58]\tvalid_0's binary_logloss: 0.14613\tvalid_0's f1: 0.503794\n",
      "[59]\tvalid_0's binary_logloss: 0.145761\tvalid_0's f1: 0.49848\n",
      "[60]\tvalid_0's binary_logloss: 0.145441\tvalid_0's f1: 0.502283\n",
      "[61]\tvalid_0's binary_logloss: 0.145194\tvalid_0's f1: 0.5\n",
      "[62]\tvalid_0's binary_logloss: 0.144809\tvalid_0's f1: 0.503794\n",
      "[63]\tvalid_0's binary_logloss: 0.144457\tvalid_0's f1: 0.50303\n",
      "[64]\tvalid_0's binary_logloss: 0.14414\tvalid_0's f1: 0.506024\n",
      "[65]\tvalid_0's binary_logloss: 0.143872\tvalid_0's f1: 0.506787\n",
      "[66]\tvalid_0's binary_logloss: 0.14347\tvalid_0's f1: 0.509036\n",
      "[67]\tvalid_0's binary_logloss: 0.143154\tvalid_0's f1: 0.511976\n",
      "[68]\tvalid_0's binary_logloss: 0.142931\tvalid_0's f1: 0.513433\n",
      "[69]\tvalid_0's binary_logloss: 0.142679\tvalid_0's f1: 0.516418\n",
      "[70]\tvalid_0's binary_logloss: 0.142392\tvalid_0's f1: 0.514881\n",
      "[71]\tvalid_0's binary_logloss: 0.142197\tvalid_0's f1: 0.519288\n",
      "[72]\tvalid_0's binary_logloss: 0.141998\tvalid_0's f1: 0.524444\n",
      "[73]\tvalid_0's binary_logloss: 0.141798\tvalid_0's f1: 0.522895\n",
      "[74]\tvalid_0's binary_logloss: 0.141603\tvalid_0's f1: 0.52071\n",
      "[75]\tvalid_0's binary_logloss: 0.141454\tvalid_0's f1: 0.525074\n",
      "[76]\tvalid_0's binary_logloss: 0.14115\tvalid_0's f1: 0.530973\n",
      "[77]\tvalid_0's binary_logloss: 0.140986\tvalid_0's f1: 0.530973\n",
      "[78]\tvalid_0's binary_logloss: 0.1408\tvalid_0's f1: 0.528804\n",
      "[79]\tvalid_0's binary_logloss: 0.140497\tvalid_0's f1: 0.525849\n",
      "[80]\tvalid_0's binary_logloss: 0.140308\tvalid_0's f1: 0.525849\n",
      "[81]\tvalid_0's binary_logloss: 0.140102\tvalid_0's f1: 0.525849\n",
      "[82]\tvalid_0's binary_logloss: 0.139967\tvalid_0's f1: 0.528024\n",
      "[83]\tvalid_0's binary_logloss: 0.139841\tvalid_0's f1: 0.535294\n",
      "[84]\tvalid_0's binary_logloss: 0.13973\tvalid_0's f1: 0.532353\n",
      "[85]\tvalid_0's binary_logloss: 0.139705\tvalid_0's f1: 0.529412\n",
      "[86]\tvalid_0's binary_logloss: 0.139448\tvalid_0's f1: 0.534508\n",
      "[87]\tvalid_0's binary_logloss: 0.13926\tvalid_0's f1: 0.536657\n",
      "[88]\tvalid_0's binary_logloss: 0.139121\tvalid_0's f1: 0.535871\n",
      "[89]\tvalid_0's binary_logloss: 0.139002\tvalid_0's f1: 0.533724\n",
      "[90]\tvalid_0's binary_logloss: 0.13879\tvalid_0's f1: 0.531571\n",
      "[91]\tvalid_0's binary_logloss: 0.138532\tvalid_0's f1: 0.532164\n",
      "[92]\tvalid_0's binary_logloss: 0.138363\tvalid_0's f1: 0.532164\n",
      "[93]\tvalid_0's binary_logloss: 0.138165\tvalid_0's f1: 0.534307\n",
      "[94]\tvalid_0's binary_logloss: 0.137859\tvalid_0's f1: 0.537226\n",
      "[95]\tvalid_0's binary_logloss: 0.137667\tvalid_0's f1: 0.539359\n",
      "[96]\tvalid_0's binary_logloss: 0.137647\tvalid_0's f1: 0.536443\n",
      "[97]\tvalid_0's binary_logloss: 0.137467\tvalid_0's f1: 0.539359\n",
      "[98]\tvalid_0's binary_logloss: 0.137368\tvalid_0's f1: 0.539359\n",
      "[99]\tvalid_0's binary_logloss: 0.137209\tvalid_0's f1: 0.538574\n",
      "[100]\tvalid_0's binary_logloss: 0.137115\tvalid_0's f1: 0.539913\n",
      "[101]\tvalid_0's binary_logloss: 0.136961\tvalid_0's f1: 0.542029\n",
      "[102]\tvalid_0's binary_logloss: 0.136877\tvalid_0's f1: 0.542029\n",
      "[103]\tvalid_0's binary_logloss: 0.136777\tvalid_0's f1: 0.547033\n",
      "[104]\tvalid_0's binary_logloss: 0.136622\tvalid_0's f1: 0.547033\n",
      "[105]\tvalid_0's binary_logloss: 0.136477\tvalid_0's f1: 0.539913\n",
      "[106]\tvalid_0's binary_logloss: 0.136359\tvalid_0's f1: 0.545718\n",
      "[107]\tvalid_0's binary_logloss: 0.136172\tvalid_0's f1: 0.543605\n",
      "[108]\tvalid_0's binary_logloss: 0.136158\tvalid_0's f1: 0.543605\n",
      "[109]\tvalid_0's binary_logloss: 0.135993\tvalid_0's f1: 0.544396\n",
      "[110]\tvalid_0's binary_logloss: 0.135892\tvalid_0's f1: 0.546512\n",
      "[111]\tvalid_0's binary_logloss: 0.135906\tvalid_0's f1: 0.548621\n",
      "[112]\tvalid_0's binary_logloss: 0.135846\tvalid_0's f1: 0.544396\n",
      "[113]\tvalid_0's binary_logloss: 0.135866\tvalid_0's f1: 0.548621\n",
      "[114]\tvalid_0's binary_logloss: 0.135789\tvalid_0's f1: 0.550725\n",
      "[115]\tvalid_0's binary_logloss: 0.135687\tvalid_0's f1: 0.550725\n",
      "[116]\tvalid_0's binary_logloss: 0.135566\tvalid_0's f1: 0.551524\n",
      "[117]\tvalid_0's binary_logloss: 0.135441\tvalid_0's f1: 0.552822\n",
      "[118]\tvalid_0's binary_logloss: 0.135471\tvalid_0's f1: 0.552023\n",
      "[119]\tvalid_0's binary_logloss: 0.135524\tvalid_0's f1: 0.554113\n",
      "[120]\tvalid_0's binary_logloss: 0.135379\tvalid_0's f1: 0.552822\n",
      "[121]\tvalid_0's binary_logloss: 0.135294\tvalid_0's f1: 0.557471\n",
      "[122]\tvalid_0's binary_logloss: 0.13523\tvalid_0's f1: 0.56528\n",
      "[123]\tvalid_0's binary_logloss: 0.135157\tvalid_0's f1: 0.56447\n",
      "[124]\tvalid_0's binary_logloss: 0.135048\tvalid_0's f1: 0.561151\n",
      "[125]\tvalid_0's binary_logloss: 0.134844\tvalid_0's f1: 0.564029\n",
      "[126]\tvalid_0's binary_logloss: 0.134753\tvalid_0's f1: 0.567335\n",
      "[127]\tvalid_0's binary_logloss: 0.134773\tvalid_0's f1: 0.568149\n",
      "[128]\tvalid_0's binary_logloss: 0.134796\tvalid_0's f1: 0.56528\n",
      "[129]\tvalid_0's binary_logloss: 0.134758\tvalid_0's f1: 0.566524\n",
      "[130]\tvalid_0's binary_logloss: 0.134691\tvalid_0's f1: 0.566524\n",
      "[131]\tvalid_0's binary_logloss: 0.134548\tvalid_0's f1: 0.56528\n",
      "[132]\tvalid_0's binary_logloss: 0.134471\tvalid_0's f1: 0.56528\n",
      "[133]\tvalid_0's binary_logloss: 0.134389\tvalid_0's f1: 0.56241\n",
      "[134]\tvalid_0's binary_logloss: 0.134286\tvalid_0's f1: 0.56447\n",
      "[135]\tvalid_0's binary_logloss: 0.134153\tvalid_0's f1: 0.566524\n",
      "[136]\tvalid_0's binary_logloss: 0.133975\tvalid_0's f1: 0.566524\n",
      "[137]\tvalid_0's binary_logloss: 0.133905\tvalid_0's f1: 0.56447\n",
      "[138]\tvalid_0's binary_logloss: 0.133815\tvalid_0's f1: 0.568571\n",
      "[139]\tvalid_0's binary_logloss: 0.133768\tvalid_0's f1: 0.564029\n",
      "[140]\tvalid_0's binary_logloss: 0.133649\tvalid_0's f1: 0.566524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141]\tvalid_0's binary_logloss: 0.133462\tvalid_0's f1: 0.57265\n",
      "[142]\tvalid_0's binary_logloss: 0.133264\tvalid_0's f1: 0.571429\n",
      "[143]\tvalid_0's binary_logloss: 0.133207\tvalid_0's f1: 0.572246\n",
      "[144]\tvalid_0's binary_logloss: 0.133082\tvalid_0's f1: 0.572246\n",
      "[145]\tvalid_0's binary_logloss: 0.132957\tvalid_0's f1: 0.573466\n",
      "[146]\tvalid_0's binary_logloss: 0.133055\tvalid_0's f1: 0.575887\n",
      "[147]\tvalid_0's binary_logloss: 0.133001\tvalid_0's f1: 0.57265\n",
      "[148]\tvalid_0's binary_logloss: 0.132944\tvalid_0's f1: 0.573864\n",
      "[149]\tvalid_0's binary_logloss: 0.132908\tvalid_0's f1: 0.571835\n",
      "[150]\tvalid_0's binary_logloss: 0.132887\tvalid_0's f1: 0.571023\n",
      "[151]\tvalid_0's binary_logloss: 0.132858\tvalid_0's f1: 0.57305\n",
      "[152]\tvalid_0's binary_logloss: 0.132798\tvalid_0's f1: 0.575887\n",
      "[153]\tvalid_0's binary_logloss: 0.132804\tvalid_0's f1: 0.58037\n",
      "[154]\tvalid_0's binary_logloss: 0.132745\tvalid_0's f1: 0.579545\n",
      "[155]\tvalid_0's binary_logloss: 0.132763\tvalid_0's f1: 0.580737\n",
      "[156]\tvalid_0's binary_logloss: 0.132683\tvalid_0's f1: 0.58156\n",
      "[157]\tvalid_0's binary_logloss: 0.132585\tvalid_0's f1: 0.58156\n",
      "[158]\tvalid_0's binary_logloss: 0.132507\tvalid_0's f1: 0.582386\n",
      "[159]\tvalid_0's binary_logloss: 0.132386\tvalid_0's f1: 0.582386\n",
      "[160]\tvalid_0's binary_logloss: 0.13232\tvalid_0's f1: 0.586402\n",
      "[161]\tvalid_0's binary_logloss: 0.132378\tvalid_0's f1: 0.587234\n",
      "[162]\tvalid_0's binary_logloss: 0.132368\tvalid_0's f1: 0.584397\n",
      "[163]\tvalid_0's binary_logloss: 0.132178\tvalid_0's f1: 0.585227\n",
      "[164]\tvalid_0's binary_logloss: 0.132033\tvalid_0's f1: 0.584397\n",
      "[165]\tvalid_0's binary_logloss: 0.131999\tvalid_0's f1: 0.585573\n",
      "[166]\tvalid_0's binary_logloss: 0.132014\tvalid_0's f1: 0.589235\n",
      "[167]\tvalid_0's binary_logloss: 0.132019\tvalid_0's f1: 0.587571\n",
      "[168]\tvalid_0's binary_logloss: 0.131975\tvalid_0's f1: 0.588402\n",
      "[169]\tvalid_0's binary_logloss: 0.131884\tvalid_0's f1: 0.597183\n",
      "[170]\tvalid_0's binary_logloss: 0.131813\tvalid_0's f1: 0.599156\n",
      "[171]\tvalid_0's binary_logloss: 0.131772\tvalid_0's f1: 0.597183\n",
      "[172]\tvalid_0's binary_logloss: 0.131741\tvalid_0's f1: 0.601124\n",
      "[173]\tvalid_0's binary_logloss: 0.131607\tvalid_0's f1: 0.597183\n",
      "[174]\tvalid_0's binary_logloss: 0.131555\tvalid_0's f1: 0.597183\n",
      "[175]\tvalid_0's binary_logloss: 0.131502\tvalid_0's f1: 0.597183\n",
      "[176]\tvalid_0's binary_logloss: 0.131417\tvalid_0's f1: 0.595205\n",
      "[177]\tvalid_0's binary_logloss: 0.131322\tvalid_0's f1: 0.599156\n",
      "[178]\tvalid_0's binary_logloss: 0.131153\tvalid_0's f1: 0.597183\n",
      "[179]\tvalid_0's binary_logloss: 0.131138\tvalid_0's f1: 0.597183\n",
      "[180]\tvalid_0's binary_logloss: 0.130982\tvalid_0's f1: 0.599156\n",
      "[181]\tvalid_0's binary_logloss: 0.130993\tvalid_0's f1: 0.600281\n",
      "[182]\tvalid_0's binary_logloss: 0.130978\tvalid_0's f1: 0.600281\n",
      "[183]\tvalid_0's binary_logloss: 0.13095\tvalid_0's f1: 0.600281\n",
      "[184]\tvalid_0's binary_logloss: 0.130878\tvalid_0's f1: 0.604196\n",
      "[185]\tvalid_0's binary_logloss: 0.130861\tvalid_0's f1: 0.608939\n",
      "[186]\tvalid_0's binary_logloss: 0.130821\tvalid_0's f1: 0.608089\n",
      "[187]\tvalid_0's binary_logloss: 0.130845\tvalid_0's f1: 0.608089\n",
      "[188]\tvalid_0's binary_logloss: 0.130901\tvalid_0's f1: 0.606993\n",
      "[189]\tvalid_0's binary_logloss: 0.130813\tvalid_0's f1: 0.608939\n",
      "[190]\tvalid_0's binary_logloss: 0.130774\tvalid_0's f1: 0.608939\n",
      "[191]\tvalid_0's binary_logloss: 0.13072\tvalid_0's f1: 0.606993\n",
      "[192]\tvalid_0's binary_logloss: 0.130656\tvalid_0's f1: 0.606993\n",
      "[193]\tvalid_0's binary_logloss: 0.130617\tvalid_0's f1: 0.606145\n",
      "[194]\tvalid_0's binary_logloss: 0.130644\tvalid_0's f1: 0.608089\n",
      "[195]\tvalid_0's binary_logloss: 0.130653\tvalid_0's f1: 0.600281\n",
      "[196]\tvalid_0's binary_logloss: 0.13062\tvalid_0's f1: 0.601124\n",
      "[197]\tvalid_0's binary_logloss: 0.130487\tvalid_0's f1: 0.603086\n",
      "[198]\tvalid_0's binary_logloss: 0.130435\tvalid_0's f1: 0.603086\n",
      "[199]\tvalid_0's binary_logloss: 0.130463\tvalid_0's f1: 0.603086\n",
      "[200]\tvalid_0's binary_logloss: 0.130444\tvalid_0's f1: 0.603086\n",
      "[201]\tvalid_0's binary_logloss: 0.130486\tvalid_0's f1: 0.60979\n",
      "[202]\tvalid_0's binary_logloss: 0.130506\tvalid_0's f1: 0.608089\n",
      "[203]\tvalid_0's binary_logloss: 0.130463\tvalid_0's f1: 0.60979\n",
      "[204]\tvalid_0's binary_logloss: 0.130318\tvalid_0's f1: 0.611732\n",
      "[205]\tvalid_0's binary_logloss: 0.130389\tvalid_0's f1: 0.611732\n",
      "[206]\tvalid_0's binary_logloss: 0.13032\tvalid_0's f1: 0.613668\n",
      "[207]\tvalid_0's binary_logloss: 0.130301\tvalid_0's f1: 0.617524\n",
      "[208]\tvalid_0's binary_logloss: 0.130292\tvalid_0's f1: 0.617524\n",
      "[209]\tvalid_0's binary_logloss: 0.130313\tvalid_0's f1: 0.619247\n",
      "[210]\tvalid_0's binary_logloss: 0.130305\tvalid_0's f1: 0.619247\n",
      "[211]\tvalid_0's binary_logloss: 0.130294\tvalid_0's f1: 0.619247\n",
      "[212]\tvalid_0's binary_logloss: 0.130216\tvalid_0's f1: 0.620112\n",
      "[213]\tvalid_0's binary_logloss: 0.130205\tvalid_0's f1: 0.620112\n",
      "[214]\tvalid_0's binary_logloss: 0.130131\tvalid_0's f1: 0.622036\n",
      "[215]\tvalid_0's binary_logloss: 0.130188\tvalid_0's f1: 0.622036\n",
      "[216]\tvalid_0's binary_logloss: 0.130193\tvalid_0's f1: 0.622036\n",
      "[217]\tvalid_0's binary_logloss: 0.130135\tvalid_0's f1: 0.62117\n",
      "[218]\tvalid_0's binary_logloss: 0.130179\tvalid_0's f1: 0.619247\n",
      "[219]\tvalid_0's binary_logloss: 0.130227\tvalid_0's f1: 0.618384\n",
      "[220]\tvalid_0's binary_logloss: 0.130244\tvalid_0's f1: 0.619247\n",
      "[221]\tvalid_0's binary_logloss: 0.130182\tvalid_0's f1: 0.620112\n",
      "[222]\tvalid_0's binary_logloss: 0.130116\tvalid_0's f1: 0.618182\n",
      "[223]\tvalid_0's binary_logloss: 0.1301\tvalid_0's f1: 0.616246\n",
      "[224]\tvalid_0's binary_logloss: 0.130083\tvalid_0's f1: 0.618182\n",
      "[225]\tvalid_0's binary_logloss: 0.13016\tvalid_0's f1: 0.617318\n",
      "[226]\tvalid_0's binary_logloss: 0.130063\tvalid_0's f1: 0.619048\n",
      "[227]\tvalid_0's binary_logloss: 0.130059\tvalid_0's f1: 0.620979\n",
      "[228]\tvalid_0's binary_logloss: 0.129999\tvalid_0's f1: 0.625\n",
      "[229]\tvalid_0's binary_logloss: 0.130027\tvalid_0's f1: 0.624133\n",
      "[230]\tvalid_0's binary_logloss: 0.130043\tvalid_0's f1: 0.625\n",
      "[231]\tvalid_0's binary_logloss: 0.12999\tvalid_0's f1: 0.625\n",
      "[232]\tvalid_0's binary_logloss: 0.130015\tvalid_0's f1: 0.622222\n",
      "[233]\tvalid_0's binary_logloss: 0.130001\tvalid_0's f1: 0.623088\n",
      "[234]\tvalid_0's binary_logloss: 0.12999\tvalid_0's f1: 0.619247\n",
      "[235]\tvalid_0's binary_logloss: 0.130027\tvalid_0's f1: 0.618384\n",
      "[236]\tvalid_0's binary_logloss: 0.129961\tvalid_0's f1: 0.618384\n",
      "[237]\tvalid_0's binary_logloss: 0.129895\tvalid_0's f1: 0.618384\n",
      "[238]\tvalid_0's binary_logloss: 0.129916\tvalid_0's f1: 0.618384\n",
      "[239]\tvalid_0's binary_logloss: 0.130006\tvalid_0's f1: 0.617524\n",
      "[240]\tvalid_0's binary_logloss: 0.130035\tvalid_0's f1: 0.617524\n",
      "[241]\tvalid_0's binary_logloss: 0.130055\tvalid_0's f1: 0.619444\n",
      "[242]\tvalid_0's binary_logloss: 0.130165\tvalid_0's f1: 0.619444\n",
      "[243]\tvalid_0's binary_logloss: 0.130145\tvalid_0's f1: 0.619444\n",
      "[244]\tvalid_0's binary_logloss: 0.13015\tvalid_0's f1: 0.617524\n",
      "[245]\tvalid_0's binary_logloss: 0.130176\tvalid_0's f1: 0.617524\n",
      "[246]\tvalid_0's binary_logloss: 0.130218\tvalid_0's f1: 0.617524\n",
      "[247]\tvalid_0's binary_logloss: 0.130257\tvalid_0's f1: 0.617524\n",
      "[248]\tvalid_0's binary_logloss: 0.13029\tvalid_0's f1: 0.617524\n",
      "[249]\tvalid_0's binary_logloss: 0.13034\tvalid_0's f1: 0.615599\n",
      "[250]\tvalid_0's binary_logloss: 0.13033\tvalid_0's f1: 0.616457\n",
      "[251]\tvalid_0's binary_logloss: 0.13053\tvalid_0's f1: 0.618384\n",
      "[252]\tvalid_0's binary_logloss: 0.130575\tvalid_0's f1: 0.620306\n",
      "[253]\tvalid_0's binary_logloss: 0.130607\tvalid_0's f1: 0.622222\n",
      "[254]\tvalid_0's binary_logloss: 0.13058\tvalid_0's f1: 0.621359\n",
      "[255]\tvalid_0's binary_logloss: 0.130549\tvalid_0's f1: 0.623269\n",
      "[256]\tvalid_0's binary_logloss: 0.130566\tvalid_0's f1: 0.619444\n",
      "[257]\tvalid_0's binary_logloss: 0.130578\tvalid_0's f1: 0.614743\n",
      "[258]\tvalid_0's binary_logloss: 0.130538\tvalid_0's f1: 0.616667\n",
      "[259]\tvalid_0's binary_logloss: 0.130541\tvalid_0's f1: 0.616667\n",
      "[260]\tvalid_0's binary_logloss: 0.130614\tvalid_0's f1: 0.613889\n",
      "[261]\tvalid_0's binary_logloss: 0.130584\tvalid_0's f1: 0.610028\n",
      "[262]\tvalid_0's binary_logloss: 0.130636\tvalid_0's f1: 0.615811\n",
      "[263]\tvalid_0's binary_logloss: 0.130613\tvalid_0's f1: 0.611961\n",
      "[264]\tvalid_0's binary_logloss: 0.130627\tvalid_0's f1: 0.614743\n",
      "[265]\tvalid_0's binary_logloss: 0.130689\tvalid_0's f1: 0.611961\n",
      "[266]\tvalid_0's binary_logloss: 0.130751\tvalid_0's f1: 0.614743\n",
      "[267]\tvalid_0's binary_logloss: 0.130744\tvalid_0's f1: 0.614743\n",
      "[268]\tvalid_0's binary_logloss: 0.130751\tvalid_0's f1: 0.614743\n",
      "[269]\tvalid_0's binary_logloss: 0.130733\tvalid_0's f1: 0.612813\n",
      "[270]\tvalid_0's binary_logloss: 0.13068\tvalid_0's f1: 0.613889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[271]\tvalid_0's binary_logloss: 0.130762\tvalid_0's f1: 0.614743\n",
      "[272]\tvalid_0's binary_logloss: 0.130783\tvalid_0's f1: 0.616667\n",
      "[273]\tvalid_0's binary_logloss: 0.130839\tvalid_0's f1: 0.616667\n",
      "[274]\tvalid_0's binary_logloss: 0.130855\tvalid_0's f1: 0.613037\n",
      "[275]\tvalid_0's binary_logloss: 0.130814\tvalid_0's f1: 0.613889\n",
      "[276]\tvalid_0's binary_logloss: 0.130854\tvalid_0's f1: 0.616667\n",
      "[277]\tvalid_0's binary_logloss: 0.130864\tvalid_0's f1: 0.615599\n",
      "[278]\tvalid_0's binary_logloss: 0.130894\tvalid_0's f1: 0.617729\n",
      "[279]\tvalid_0's binary_logloss: 0.131024\tvalid_0's f1: 0.617729\n",
      "[280]\tvalid_0's binary_logloss: 0.131099\tvalid_0's f1: 0.618585\n",
      "[281]\tvalid_0's binary_logloss: 0.131031\tvalid_0's f1: 0.618585\n",
      "[282]\tvalid_0's binary_logloss: 0.131041\tvalid_0's f1: 0.615811\n",
      "[283]\tvalid_0's binary_logloss: 0.130998\tvalid_0's f1: 0.620499\n",
      "[284]\tvalid_0's binary_logloss: 0.130902\tvalid_0's f1: 0.624309\n",
      "[285]\tvalid_0's binary_logloss: 0.130926\tvalid_0's f1: 0.624309\n",
      "[286]\tvalid_0's binary_logloss: 0.130895\tvalid_0's f1: 0.624309\n",
      "[287]\tvalid_0's binary_logloss: 0.130862\tvalid_0's f1: 0.624309\n",
      "[288]\tvalid_0's binary_logloss: 0.130926\tvalid_0's f1: 0.623448\n",
      "[289]\tvalid_0's binary_logloss: 0.130974\tvalid_0's f1: 0.623448\n",
      "[290]\tvalid_0's binary_logloss: 0.131034\tvalid_0's f1: 0.626207\n",
      "[291]\tvalid_0's binary_logloss: 0.131061\tvalid_0's f1: 0.624309\n",
      "[292]\tvalid_0's binary_logloss: 0.131014\tvalid_0's f1: 0.627072\n",
      "[293]\tvalid_0's binary_logloss: 0.131083\tvalid_0's f1: 0.625173\n",
      "[294]\tvalid_0's binary_logloss: 0.131132\tvalid_0's f1: 0.625173\n",
      "[295]\tvalid_0's binary_logloss: 0.131285\tvalid_0's f1: 0.622407\n",
      "[296]\tvalid_0's binary_logloss: 0.131272\tvalid_0's f1: 0.619444\n",
      "[297]\tvalid_0's binary_logloss: 0.131222\tvalid_0's f1: 0.617524\n",
      "[298]\tvalid_0's binary_logloss: 0.13123\tvalid_0's f1: 0.617524\n",
      "[299]\tvalid_0's binary_logloss: 0.131255\tvalid_0's f1: 0.617524\n",
      "[300]\tvalid_0's binary_logloss: 0.131288\tvalid_0's f1: 0.620499\n",
      "[301]\tvalid_0's binary_logloss: 0.131244\tvalid_0's f1: 0.623269\n",
      "[302]\tvalid_0's binary_logloss: 0.131275\tvalid_0's f1: 0.623269\n",
      "[303]\tvalid_0's binary_logloss: 0.131338\tvalid_0's f1: 0.622407\n",
      "[304]\tvalid_0's binary_logloss: 0.131325\tvalid_0's f1: 0.623269\n",
      "[305]\tvalid_0's binary_logloss: 0.131308\tvalid_0's f1: 0.626039\n",
      "[306]\tvalid_0's binary_logloss: 0.131272\tvalid_0's f1: 0.626039\n",
      "[307]\tvalid_0's binary_logloss: 0.131276\tvalid_0's f1: 0.626039\n",
      "[308]\tvalid_0's binary_logloss: 0.13136\tvalid_0's f1: 0.625173\n",
      "[309]\tvalid_0's binary_logloss: 0.131329\tvalid_0's f1: 0.625173\n",
      "[310]\tvalid_0's binary_logloss: 0.13135\tvalid_0's f1: 0.626207\n",
      "[311]\tvalid_0's binary_logloss: 0.131449\tvalid_0's f1: 0.627235\n",
      "[312]\tvalid_0's binary_logloss: 0.131477\tvalid_0's f1: 0.626207\n",
      "[313]\tvalid_0's binary_logloss: 0.13154\tvalid_0's f1: 0.623448\n",
      "[314]\tvalid_0's binary_logloss: 0.131607\tvalid_0's f1: 0.621547\n",
      "[315]\tvalid_0's binary_logloss: 0.131635\tvalid_0's f1: 0.623448\n",
      "[316]\tvalid_0's binary_logloss: 0.131603\tvalid_0's f1: 0.624484\n",
      "[317]\tvalid_0's binary_logloss: 0.131465\tvalid_0's f1: 0.627235\n",
      "[318]\tvalid_0's binary_logloss: 0.131399\tvalid_0's f1: 0.627072\n",
      "[319]\tvalid_0's binary_logloss: 0.131431\tvalid_0's f1: 0.629986\n",
      "[320]\tvalid_0's binary_logloss: 0.131418\tvalid_0's f1: 0.628099\n",
      "[321]\tvalid_0's binary_logloss: 0.13143\tvalid_0's f1: 0.629986\n",
      "[322]\tvalid_0's binary_logloss: 0.131502\tvalid_0's f1: 0.629986\n",
      "[323]\tvalid_0's binary_logloss: 0.131605\tvalid_0's f1: 0.628099\n",
      "[324]\tvalid_0's binary_logloss: 0.131605\tvalid_0's f1: 0.627235\n",
      "[325]\tvalid_0's binary_logloss: 0.13165\tvalid_0's f1: 0.627235\n",
      "[326]\tvalid_0's binary_logloss: 0.131798\tvalid_0's f1: 0.628099\n",
      "[327]\tvalid_0's binary_logloss: 0.131869\tvalid_0's f1: 0.627235\n",
      "[328]\tvalid_0's binary_logloss: 0.131899\tvalid_0's f1: 0.627235\n",
      "[329]\tvalid_0's binary_logloss: 0.131954\tvalid_0's f1: 0.626207\n",
      "[330]\tvalid_0's binary_logloss: 0.132\tvalid_0's f1: 0.625344\n",
      "[331]\tvalid_0's binary_logloss: 0.132091\tvalid_0's f1: 0.625344\n",
      "[332]\tvalid_0's binary_logloss: 0.132107\tvalid_0's f1: 0.626207\n",
      "[333]\tvalid_0's binary_logloss: 0.132198\tvalid_0's f1: 0.624309\n",
      "[334]\tvalid_0's binary_logloss: 0.132237\tvalid_0's f1: 0.624309\n",
      "[335]\tvalid_0's binary_logloss: 0.132331\tvalid_0's f1: 0.622407\n",
      "[336]\tvalid_0's binary_logloss: 0.132415\tvalid_0's f1: 0.621547\n",
      "[337]\tvalid_0's binary_logloss: 0.132405\tvalid_0's f1: 0.623448\n",
      "[338]\tvalid_0's binary_logloss: 0.132545\tvalid_0's f1: 0.622407\n",
      "[339]\tvalid_0's binary_logloss: 0.132546\tvalid_0's f1: 0.622407\n",
      "[340]\tvalid_0's binary_logloss: 0.132551\tvalid_0's f1: 0.623448\n",
      "[341]\tvalid_0's binary_logloss: 0.132553\tvalid_0's f1: 0.623448\n",
      "[342]\tvalid_0's binary_logloss: 0.132519\tvalid_0's f1: 0.623448\n",
      "[343]\tvalid_0's binary_logloss: 0.132482\tvalid_0's f1: 0.623448\n",
      "[344]\tvalid_0's binary_logloss: 0.132513\tvalid_0's f1: 0.621547\n",
      "[345]\tvalid_0's binary_logloss: 0.132555\tvalid_0's f1: 0.62069\n",
      "[346]\tvalid_0's binary_logloss: 0.132586\tvalid_0's f1: 0.62069\n",
      "[347]\tvalid_0's binary_logloss: 0.132628\tvalid_0's f1: 0.621733\n",
      "[348]\tvalid_0's binary_logloss: 0.132589\tvalid_0's f1: 0.623448\n",
      "[349]\tvalid_0's binary_logloss: 0.132646\tvalid_0's f1: 0.623448\n",
      "[350]\tvalid_0's binary_logloss: 0.13271\tvalid_0's f1: 0.625344\n",
      "[351]\tvalid_0's binary_logloss: 0.132725\tvalid_0's f1: 0.625344\n",
      "[352]\tvalid_0's binary_logloss: 0.132678\tvalid_0's f1: 0.625344\n",
      "[353]\tvalid_0's binary_logloss: 0.132711\tvalid_0's f1: 0.625344\n",
      "[354]\tvalid_0's binary_logloss: 0.132699\tvalid_0's f1: 0.624484\n",
      "[355]\tvalid_0's binary_logloss: 0.132777\tvalid_0's f1: 0.62259\n",
      "[356]\tvalid_0's binary_logloss: 0.132866\tvalid_0's f1: 0.621733\n",
      "[357]\tvalid_0's binary_logloss: 0.1329\tvalid_0's f1: 0.621733\n",
      "[358]\tvalid_0's binary_logloss: 0.132991\tvalid_0's f1: 0.619835\n",
      "[359]\tvalid_0's binary_logloss: 0.132969\tvalid_0's f1: 0.618982\n",
      "[360]\tvalid_0's binary_logloss: 0.132956\tvalid_0's f1: 0.618982\n",
      "[361]\tvalid_0's binary_logloss: 0.132969\tvalid_0's f1: 0.618982\n",
      "[362]\tvalid_0's binary_logloss: 0.13301\tvalid_0's f1: 0.618982\n",
      "[363]\tvalid_0's binary_logloss: 0.133004\tvalid_0's f1: 0.620879\n",
      "[364]\tvalid_0's binary_logloss: 0.133042\tvalid_0's f1: 0.620879\n",
      "[365]\tvalid_0's binary_logloss: 0.133034\tvalid_0's f1: 0.620879\n",
      "[366]\tvalid_0's binary_logloss: 0.132959\tvalid_0's f1: 0.623448\n",
      "[367]\tvalid_0's binary_logloss: 0.133011\tvalid_0's f1: 0.62259\n",
      "[368]\tvalid_0's binary_logloss: 0.132998\tvalid_0's f1: 0.621733\n",
      "[369]\tvalid_0's binary_logloss: 0.132987\tvalid_0's f1: 0.621733\n",
      "[370]\tvalid_0's binary_logloss: 0.132923\tvalid_0's f1: 0.621733\n",
      "[371]\tvalid_0's binary_logloss: 0.132922\tvalid_0's f1: 0.621733\n",
      "[372]\tvalid_0's binary_logloss: 0.132924\tvalid_0's f1: 0.621733\n",
      "[373]\tvalid_0's binary_logloss: 0.132927\tvalid_0's f1: 0.621733\n",
      "[374]\tvalid_0's binary_logloss: 0.132981\tvalid_0's f1: 0.621733\n",
      "[375]\tvalid_0's binary_logloss: 0.13302\tvalid_0's f1: 0.621733\n",
      "[376]\tvalid_0's binary_logloss: 0.133074\tvalid_0's f1: 0.62259\n",
      "[377]\tvalid_0's binary_logloss: 0.133174\tvalid_0's f1: 0.621733\n",
      "[378]\tvalid_0's binary_logloss: 0.133216\tvalid_0's f1: 0.621733\n",
      "[379]\tvalid_0's binary_logloss: 0.13333\tvalid_0's f1: 0.619835\n",
      "[380]\tvalid_0's binary_logloss: 0.133406\tvalid_0's f1: 0.619835\n",
      "[381]\tvalid_0's binary_logloss: 0.133331\tvalid_0's f1: 0.62069\n",
      "[382]\tvalid_0's binary_logloss: 0.133271\tvalid_0's f1: 0.61708\n",
      "[383]\tvalid_0's binary_logloss: 0.133275\tvalid_0's f1: 0.618785\n",
      "[384]\tvalid_0's binary_logloss: 0.133304\tvalid_0's f1: 0.618785\n",
      "[385]\tvalid_0's binary_logloss: 0.13335\tvalid_0's f1: 0.618785\n",
      "[386]\tvalid_0's binary_logloss: 0.133451\tvalid_0's f1: 0.62069\n",
      "[387]\tvalid_0's binary_logloss: 0.133519\tvalid_0's f1: 0.619835\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.129895\tvalid_0's f1: 0.618384\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_eval = lgb.Dataset(x_valid, y_valid, reference = lgb_train)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 35,\n",
    "    'max_depth': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=lgb_eval,\n",
    "                feval = lgb_f1_score,\n",
    "                evals_result=evals_result,\n",
    "                early_stopping_rounds=150)\n",
    "# lgb.plot_metric(evals_result, metric='f1')\n",
    "pred_lgb = gbm.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lgb = np.round(pred_lgb)\n",
    "pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6364</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6366</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6372</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6373</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6374</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6376</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6377</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6380</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6381</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6382</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6384</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6385</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6386</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6387</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6388</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6389</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6393 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "5     0.0\n",
       "6     0.0\n",
       "7     0.0\n",
       "8     0.0\n",
       "9     0.0\n",
       "10    0.0\n",
       "11    0.0\n",
       "12    0.0\n",
       "13    0.0\n",
       "14    0.0\n",
       "15    0.0\n",
       "16    0.0\n",
       "17    0.0\n",
       "18    0.0\n",
       "19    0.0\n",
       "20    0.0\n",
       "21    0.0\n",
       "22    0.0\n",
       "23    0.0\n",
       "24    0.0\n",
       "25    0.0\n",
       "26    0.0\n",
       "27    0.0\n",
       "28    0.0\n",
       "29    0.0\n",
       "...   ...\n",
       "6363  0.0\n",
       "6364  0.0\n",
       "6365  0.0\n",
       "6366  0.0\n",
       "6367  0.0\n",
       "6368  0.0\n",
       "6369  0.0\n",
       "6370  0.0\n",
       "6371  0.0\n",
       "6372  0.0\n",
       "6373  1.0\n",
       "6374  0.0\n",
       "6375  0.0\n",
       "6376  0.0\n",
       "6377  0.0\n",
       "6378  0.0\n",
       "6379  0.0\n",
       "6380  0.0\n",
       "6381  0.0\n",
       "6382  0.0\n",
       "6383  0.0\n",
       "6384  0.0\n",
       "6385  0.0\n",
       "6386  0.0\n",
       "6387  0.0\n",
       "6388  0.0\n",
       "6389  0.0\n",
       "6390  0.0\n",
       "6391  0.0\n",
       "6392  0.0\n",
       "\n",
       "[6393 rows x 1 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbt = pd.DataFrame(pred_lgb)\n",
    "lgbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.928672\teval-f1_dcore:0.133158\n",
      "Multiple eval metrics have been passed: 'eval-f1_dcore' will be used for early stopping.\n",
      "\n",
      "Will train until eval-f1_dcore hasn't improved in 100 rounds.\n",
      "[1]\teval-error:0.928672\teval-f1_dcore:0.133158\n",
      "[2]\teval-error:0.17441\teval-f1_dcore:0.344503\n",
      "[3]\teval-error:0.06163\teval-f1_dcore:0.458791\n",
      "[4]\teval-error:0.058971\teval-f1_dcore:0.446402\n",
      "[5]\teval-error:0.058189\teval-f1_dcore:0.411392\n",
      "[6]\teval-error:0.058501\teval-f1_dcore:0.392857\n",
      "[7]\teval-error:0.058032\teval-f1_dcore:0.396748\n",
      "[8]\teval-error:0.057719\teval-f1_dcore:0.398042\n",
      "[9]\teval-error:0.057407\teval-f1_dcore:0.399345\n",
      "[10]\teval-error:0.057876\teval-f1_dcore:0.391447\n",
      "[11]\teval-error:0.057094\teval-f1_dcore:0.402619\n",
      "[12]\teval-error:0.056312\teval-f1_dcore:0.411765\n",
      "[13]\teval-error:0.056155\teval-f1_dcore:0.41626\n",
      "[14]\teval-error:0.055842\teval-f1_dcore:0.421394\n",
      "[15]\teval-error:0.054591\teval-f1_dcore:0.4416\n",
      "[16]\teval-error:0.05506\teval-f1_dcore:0.443038\n",
      "[17]\teval-error:0.053965\teval-f1_dcore:0.460094\n",
      "[18]\teval-error:0.053809\teval-f1_dcore:0.4625\n",
      "[19]\teval-error:0.052557\teval-f1_dcore:0.483077\n",
      "[20]\teval-error:0.053027\teval-f1_dcore:0.479263\n",
      "[21]\teval-error:0.05287\teval-f1_dcore:0.481595\n",
      "[22]\teval-error:0.052401\teval-f1_dcore:0.491654\n",
      "[23]\teval-error:0.053027\teval-f1_dcore:0.487141\n",
      "[24]\teval-error:0.05287\teval-f1_dcore:0.489426\n",
      "[25]\teval-error:0.051775\teval-f1_dcore:0.502256\n",
      "[26]\teval-error:0.050993\teval-f1_dcore:0.511976\n",
      "[27]\teval-error:0.05068\teval-f1_dcore:0.519288\n",
      "[28]\teval-error:0.050524\teval-f1_dcore:0.521481\n",
      "[29]\teval-error:0.049742\teval-f1_dcore:0.530973\n",
      "[30]\teval-error:0.049585\teval-f1_dcore:0.535871\n",
      "[31]\teval-error:0.050211\teval-f1_dcore:0.532751\n",
      "[32]\teval-error:0.049742\teval-f1_dcore:0.537791\n",
      "[33]\teval-error:0.050055\teval-f1_dcore:0.532164\n",
      "[34]\teval-error:0.049273\teval-f1_dcore:0.540146\n",
      "[35]\teval-error:0.04896\teval-f1_dcore:0.548341\n",
      "[36]\teval-error:0.049585\teval-f1_dcore:0.546495\n",
      "[37]\teval-error:0.049429\teval-f1_dcore:0.549858\n",
      "[38]\teval-error:0.049585\teval-f1_dcore:0.547789\n",
      "[39]\teval-error:0.049429\teval-f1_dcore:0.549858\n",
      "[40]\teval-error:0.04896\teval-f1_dcore:0.556028\n",
      "[41]\teval-error:0.049116\teval-f1_dcore:0.552707\n",
      "[42]\teval-error:0.049116\teval-f1_dcore:0.552707\n",
      "[43]\teval-error:0.049429\teval-f1_dcore:0.552408\n",
      "[44]\teval-error:0.048803\teval-f1_dcore:0.561798\n",
      "[45]\teval-error:0.049742\teval-f1_dcore:0.558333\n",
      "[46]\teval-error:0.049742\teval-f1_dcore:0.560773\n",
      "[47]\teval-error:0.049742\teval-f1_dcore:0.558333\n",
      "[48]\teval-error:0.049585\teval-f1_dcore:0.55911\n",
      "[49]\teval-error:0.050055\teval-f1_dcore:0.562842\n",
      "[50]\teval-error:0.049429\teval-f1_dcore:0.568306\n",
      "[51]\teval-error:0.049429\teval-f1_dcore:0.565934\n",
      "[52]\teval-error:0.049742\teval-f1_dcore:0.564384\n",
      "[53]\teval-error:0.048647\teval-f1_dcore:0.573388\n",
      "[54]\teval-error:0.048647\teval-f1_dcore:0.575716\n",
      "[55]\teval-error:0.047708\teval-f1_dcore:0.58728\n",
      "[56]\teval-error:0.047239\teval-f1_dcore:0.589674\n",
      "[57]\teval-error:0.046613\teval-f1_dcore:0.598383\n",
      "[58]\teval-error:0.047552\teval-f1_dcore:0.592493\n",
      "[59]\teval-error:0.048178\teval-f1_dcore:0.588235\n",
      "[60]\teval-error:0.048334\teval-f1_dcore:0.586345\n",
      "[61]\teval-error:0.047239\teval-f1_dcore:0.596257\n",
      "[62]\teval-error:0.048491\teval-f1_dcore:0.589947\n",
      "[63]\teval-error:0.04896\teval-f1_dcore:0.587615\n",
      "[64]\teval-error:0.049116\teval-f1_dcore:0.587927\n",
      "[65]\teval-error:0.04896\teval-f1_dcore:0.588699\n",
      "[66]\teval-error:0.050368\teval-f1_dcore:0.575198\n",
      "[67]\teval-error:0.050368\teval-f1_dcore:0.572944\n",
      "[68]\teval-error:0.049585\teval-f1_dcore:0.576769\n",
      "[69]\teval-error:0.050055\teval-f1_dcore:0.578947\n",
      "[70]\teval-error:0.04896\teval-f1_dcore:0.583222\n",
      "[71]\teval-error:0.049116\teval-f1_dcore:0.581333\n",
      "[72]\teval-error:0.048647\teval-f1_dcore:0.583668\n",
      "[73]\teval-error:0.049429\teval-f1_dcore:0.576408\n",
      "[74]\teval-error:0.049585\teval-f1_dcore:0.575636\n",
      "[75]\teval-error:0.049116\teval-f1_dcore:0.579088\n",
      "[76]\teval-error:0.049116\teval-f1_dcore:0.579088\n",
      "[77]\teval-error:0.048803\teval-f1_dcore:0.581769\n",
      "[78]\teval-error:0.048803\teval-f1_dcore:0.579515\n",
      "[79]\teval-error:0.048803\teval-f1_dcore:0.579515\n",
      "[80]\teval-error:0.048647\teval-f1_dcore:0.586985\n",
      "[81]\teval-error:0.048491\teval-f1_dcore:0.585561\n",
      "[82]\teval-error:0.048021\teval-f1_dcore:0.587919\n",
      "[83]\teval-error:0.048178\teval-f1_dcore:0.587131\n",
      "[84]\teval-error:0.048334\teval-f1_dcore:0.589641\n",
      "[85]\teval-error:0.049116\teval-f1_dcore:0.581333\n",
      "[86]\teval-error:0.049742\teval-f1_dcore:0.573727\n",
      "[87]\teval-error:0.049273\teval-f1_dcore:0.582781\n",
      "[88]\teval-error:0.049116\teval-f1_dcore:0.584656\n",
      "[89]\teval-error:0.048803\teval-f1_dcore:0.585106\n",
      "[90]\teval-error:0.049273\teval-f1_dcore:0.586071\n",
      "[91]\teval-error:0.049429\teval-f1_dcore:0.588542\n",
      "[92]\teval-error:0.049585\teval-f1_dcore:0.586701\n",
      "[93]\teval-error:0.049742\teval-f1_dcore:0.585938\n",
      "[94]\teval-error:0.049273\teval-f1_dcore:0.588235\n",
      "[95]\teval-error:0.048491\teval-f1_dcore:0.594241\n",
      "[96]\teval-error:0.048334\teval-f1_dcore:0.598179\n",
      "[97]\teval-error:0.048647\teval-f1_dcore:0.595579\n",
      "[98]\teval-error:0.049429\teval-f1_dcore:0.586387\n",
      "[99]\teval-error:0.049273\teval-f1_dcore:0.590377\n",
      "[100]\teval-error:0.049429\teval-f1_dcore:0.58961\n",
      "Stopping. Best iteration:\n",
      "[0]\teval-error:0.928672\teval-f1_dcore:0.133158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "dmatrix = xgb.DMatrix(x_train.values,\n",
    "                     y_train.values,\n",
    "                     feature_names=x_train.columns.values)\n",
    "dtest = xgb.DMatrix(x_valid.values,\n",
    "                     y_valid.values,\n",
    "                     feature_names=x_valid.columns.values)\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    score = f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_score', score\n",
    "\n",
    "evallist  = [(dtest, 'eval')]\n",
    "param = {'max_depth':8, 'min_child_weight' : 15, 'eta':0.2,\n",
    "         'reg_alpha':0.3, 'gamma':0.8, 'colsample_bytree' : 0.5, 'subsample' : 0.9,\n",
    "         'silent':False, 'objective':'binary:hinge', 'eval_metric' :'error' }\n",
    "bst = xgb.train( param, dmatrix, num_boost_round = 1000, early_stopping_rounds=100, evals=evallist, verbose_eval=True, feval = f1_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_valid = xgb.DMatrix(x_valid.values, feature_names =x_valid.columns.values )\n",
    "pred_xgb = bst.predict(d_valid)\n",
    "pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       ...,\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_level2 = np.c_[pred_lgb, pred_xgb]\n",
    "x_valid_level2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "f1_score = make_scorer(f1_score)\n",
    "tuned_parameters = {'n_estimators': [2500, 3000, 3500],\n",
    "                    'max_depth': [6, 8, 10],\n",
    "                     'min_child_weight': [4, 6, 8],\n",
    "                      'gamma': [1e-3, 0.8, 0.3],\n",
    "                     'reg_aplha': [0.3, 0.5],\n",
    "                     'learning_rate': [0.1, 0.01, 0.05]}\n",
    "                   \n",
    "from sklearn.model_selection import KFold\n",
    "xgb_cv = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                    silent=True, nthread=1, n_jobs = -1 )\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 1001).split(X=x_train, y=y_train)\n",
    "gscv = GridSearchCV(estimator = xgb_cv, param_grid = tuned_parameters, scoring = f1_score, cv = kf  )\n",
    "model = gscv.fit(X = x_train, y = y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17197\n"
     ]
    }
   ],
   "source": [
    "y_test = xg_cl.predict(x_test)\n",
    "\n",
    "y_test = list(y_test)\n",
    "print(len(y_test))\n",
    "\n",
    "\n",
    "test4 = pd.read_csv('test.csv')\n",
    "#test4.head()\n",
    "ied = list(test4['id'])\n",
    "dictionary = {'id':ied, 'label':y_test}\n",
    "df = pd.DataFrame(dictionary)\n",
    "df.head()\n",
    "df.to_csv('xgboost2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ad_cl = AdaBoostClassifier(base_estimator = RandomForestClassifier(n_estimators = 200, max_depth = 8), algorithm = 'SAMME.R', n_estimators = 100, learning_rate = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=8,\n",
       "                                                         max_features='auto',\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         n_estimators=200,\n",
       "                                                         n_jobs=None,\n",
       "                                                         oob_score=False,\n",
       "                                                         random_state=None,\n",
       "                                                         verbose=0,\n",
       "                                                         warm_start=False),\n",
       "                   learning_rate=0.1, n_estimators=100, random_state=42)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_cl.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_preds = ad_cl.predict(x_valid)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score_ = f1_score(ad_preds, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5317220543806647"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
